# Awesome Papers about Neurons in LLMs

This list focuses on neuron-level techniques in LLMs. [Another list](https://github.com/zepingyu0512/awesome-llm-understanding-mechanism.git) focuses on understanding the internal mechanism of LLMs.

Definition of neurons: A column/row in the matrix in attention or FFN layers. Please refer to "Transformer Feed-Forward Layers Are Key-Value Memories" for FFN neurons, and "Neuron-Level Knowledge Attribution in Large Language Models" for attention neurons.

Conference paper recommendation: please release a issue or contact [me](https://zepingyu0512.github.io/).

## Papers

- [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/pdf/2505.16703)
   - \[arxiv\] \[2025.5\]

- [Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models](https://arxiv.org/pdf/2502.10835)
   - \[arxiv\] \[2025.2\]
   
- [Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing](https://arxiv.org/pdf/2501.14457)
   - \[arxiv\] \[2025.1\]

- [Scaling Automatic Neuron Description](https://transluce.org/neuron-descriptions)
   - \[Transluce AI\] \[2024.10\]

- [Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis](https://zepingyu0512.github.io/arithmetic-mechanism.github.io/)
   - \[EMNLP 2024\] \[2024.9\]

- [Confidence Regulation Neurons in Language Models](https://arxiv.org/pdf/2406.16254)
   - \[NeurIPS 2024\] \[2024.6\]

- [An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs](https://arxiv.org/pdf/2406.12288)
   - \[ACL 2024\] \[2024.6\]

- [Neuron-Level Knowledge Attribution in Large Language Models](https://zepingyu0512.github.io/neuron-attribution.github.io/)
   - \[EMNLP 2024\] \[2024.6\]

- [Wasserstein Distances, Neuronal Entanglement, and Sparsity](https://arxiv.org/pdf/2405.15756)
   - \[ICLR 2025\] \[2024.5\]

- [Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models](https://arxiv.org/pdf/2402.16438)
   - \[ACL 2024\] \[2024.2\]

- [What does the Knowledge Neuron Thesis Have to do with Knowledge?](https://openreview.net/pdf?id=2HJRwwbV3G)
   - \[ICLR 2024\] \[2023.11\]

- [Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level](https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)
   - \[Deepmind\] \[2023.12\] 

- [Neurons in Large Language Models: Dead, N-gram, Positional](https://arxiv.org/pdf/2309.04827.pdf)
   - \[ACL 2024\] \[2023.9\]

- [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/pdf/2305.01610)
   - \[TMLR 2024\] \[2023.5\] 

- [Language models can explain neurons in language models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)
   - \[OpenAI\] \[2023.5\]

- [Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations](https://arxiv.org/pdf/2303.02536)
   - \[arxiv\] \[2023.5\] 

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
   - \[Anthropic\] \[2022.9\]

- [Neuron-level Interpretation of Deep NLP Models: A Survey](https://arxiv.org/pdf/2108.13138)
   - \[survey\] \[2022.8\]

- [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://arxiv.org/pdf/2203.14680.pdf)
   - \[EMNLP 2022\] \[2022.3\] 

- [Knowledge Neurons in Pretrained Transformers](https://arxiv.org/pdf/2104.08696)
   - \[ACL 2021\] \[2021.4\]

- [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/pdf/2012.14913.pdf)
   - \[EMNLP 2021\] \[2020.12\] 
