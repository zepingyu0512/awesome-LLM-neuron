# Awesome Papers about Neurons in LLMs

This list focuses on neuron-level techniques in LLMs. [Another list](https://github.com/zepingyu0512/awesome-llm-understanding-mechanism.git) focuses on understanding the internal mechanism of LLMs.

Definition of neurons: A column/row in the matrix in attention or FFN layers. Please refer to "Transformer Feed-Forward Layers Are Key-Value Memories" for FFN neurons, and "Neuron-Level Knowledge Attribution in Large Language Models" for attention neurons.

Conference paper recommendation: please release a issue or contact [me](https://zepingyu0512.github.io/).

## Papers

- [Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis](https://zepingyu0512.github.io/arithmetic-mechanism.github.io/)
   - \[EMNLP 2024\] \[2024.9\]

- [Neuron-Level Knowledge Attribution in Large Language Models](https://zepingyu0512.github.io/neuron-attribution.github.io/)
   - \[EMNLP 2024\] \[2024.6\]

- [What does the Knowledge Neuron Thesis Have to do with Knowledge?](https://openreview.net/pdf?id=2HJRwwbV3G)
   - \[ICLR 2024\] \[2023.11\]

- [Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level](https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall)
   - \[Deepmind\] \[2023.12\] 

- [Neurons in Large Language Models: Dead, N-gram, Positional](https://arxiv.org/pdf/2309.04827.pdf)
   - \[ACL 2024\] \[2023.9\]

- [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/pdf/2305.01610)
   - \[TMLR 2024\] \[2023.5\] 

- [Language models can explain neurons in language models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)
   - \[OpenAI\] \[2023.5\] 

- [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf)
   - \[2022.10\]

- [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
   - \[Anthropic\] \[2022.9\]

- [Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://arxiv.org/pdf/2203.14680.pdf)
   - \[EMNLP 2022\] \[2022.3\] 
     
- [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/pdf/2012.14913.pdf)
   - \[EMNLP 2021\] \[2020.12\] 
